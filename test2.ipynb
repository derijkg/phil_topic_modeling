{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2383225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading NLTK's 'punkt' model for sentence tokenization...\")\n",
    "    nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ae7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    end_data = []\n",
    "    with open(r'00_prep\\cleaned_texts\\all_processed.json','r',encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    for book in data:\n",
    "        for chapter in book['content']:\n",
    "            for para in chapter:\n",
    "                end_data.append(para)\n",
    "except FileNotFoundError:\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ede3950",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in end_data:\n",
    "    if not isinstance(e,str):\n",
    "        print('uhoh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0384716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (389 > 384). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3585"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pprint import pprint\n",
    "# Load the model and its tokenizer\n",
    "model_name = 'all-mpnet-base-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "lens_tokens = []\n",
    "for para in end_data:\n",
    "    tokenized = tokenizer.encode(para)\n",
    "    lens_tokens.append(len(tokenized))\n",
    "lens_tokens = sorted(lens_tokens)\n",
    "counter = 0\n",
    "for lens in lens_tokens:\n",
    "    if lens >= 384:\n",
    "        counter += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 43 books, 1655 chapters, and 20016 paragraphs.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97da986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum sequence length for  is: 384 tokens\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\", device = 'cuda')\n",
    "\n",
    "with open(r'00_prep\\cleaned_texts\\all_processed.json', 'r',encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "num_books = len(data)\n",
    "num_chapters = sum(len(book['content']) for book in data)\n",
    "num_paragraphs = sum(len(chapter) for book in data for chapter in book['content']) \n",
    "print(f\"Loaded {num_books} books, {num_chapters} chapters, and {num_paragraphs} paragraphs.\")\n",
    "\n",
    "underlying_transformer = model[0].auto_model\n",
    "tokenizer = model[0].tokenizer\n",
    "\n",
    "# Get the maximum sequence length from the model's configuration\n",
    "max_seq_length = tokenizer.model_max_length\n",
    "print(f\"The maximum sequence length for  is: {max_seq_length} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263e548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96ec7ac02504eb08418b4ba5255bc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing books:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:130122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (399 > 384). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents longer then 384: 5\n"
     ]
    }
   ],
   "source": [
    "# partitioning\n",
    "all_sentences_flat = []\n",
    "sentence_counts = []\n",
    "for book in tqdm(data, desc='Processing books'):\n",
    "    book_sentence_counts = []\n",
    "    for chapter in book['content']:\n",
    "        chapter_sentence_counts = []\n",
    "        for paragraph in chapter:\n",
    "            sentences = nltk.sent_tokenize(paragraph)\n",
    "            if sentences:\n",
    "                all_sentences_flat.extend(sentences)\n",
    "                chapter_sentence_counts.append(len(sentences))\n",
    "            else:\n",
    "                chapter_sentence_counts.append(0)\n",
    "        book_sentence_counts.append(chapter_sentence_counts)\n",
    "    sentence_counts.append(book_sentence_counts)\n",
    "\n",
    "print(f'total:{len(all_sentences_flat)}')\n",
    "counter = 0\n",
    "for sent in all_sentences_flat:\n",
    "    tokenized_sent = tokenizer.encode(sent)\n",
    "    if len(tokenized_sent) > max_seq_length:\n",
    "        counter += 1\n",
    "print(f'sents longer than {max_seq_length}: {counter}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd0e1ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04bb763",
   "metadata": {},
   "source": [
    "Sent embed and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420c274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200c306367f34f2b925ade638ffea700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing books:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:130122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50299b902784460cb3fec57a2f3b9bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1017 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c0a40789d24847a396096db67810cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "agg embeddings:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregation Complete!\n",
      "Shape of one book embedding (aggregated): (768,)\n",
      "Shape of one chapter embedding (aggregated): (768,)\n",
      "Shape of one paragraph embedding (aggregated): (768,)\n"
     ]
    }
   ],
   "source": [
    "# EMBEDDING SENTENCE LEVEL\n",
    "model_name = 'all-mpnet-base-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "with open(r'00_prep\\cleaned_texts\\all_processed.json', 'r',encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# partitioning\n",
    "all_sentences_flat = []\n",
    "sentence_counts = []\n",
    "for book in tqdm(data, desc='Processing books'):\n",
    "    book_sentence_counts = []\n",
    "    for chapter in book['content']:\n",
    "        chapter_sentence_counts = []\n",
    "        for paragraph in chapter:\n",
    "            sentences = nltk.sent_tokenize(paragraph)\n",
    "            if sentences:\n",
    "                all_sentences_flat.extend(sentences)\n",
    "                chapter_sentence_counts.append(len(sentences))\n",
    "            else:\n",
    "                chapter_sentence_counts.append(0)\n",
    "        book_sentence_counts.append(chapter_sentence_counts)\n",
    "    sentence_counts.append(book_sentence_counts)\n",
    "\n",
    "print(f'total:{len(all_sentences_flat)}')\n",
    "\n",
    "# tokenize sentences\n",
    "sentence_embeddings_flat = model.encode(\n",
    "    all_sentences_flat,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=128,\n",
    "    \n",
    ")\n",
    "\n",
    "sentence_embeddings_hierarchical = []\n",
    "paragraph_embeddings_aggregated = []\n",
    "chapter_embeddings_aggregated = []\n",
    "book_embeddings_aggregated = []\n",
    "\n",
    "current_sentence_idx = 0\n",
    "for i, book in enumerate(tqdm(data,desc='agg embeddings')):\n",
    "    book_chapter_embeddings = []\n",
    "    book_paragraph_embeddings = []\n",
    "\n",
    "    for j, chapter in enumerate(book['content']):\n",
    "        chapter_paragraph_embeddings = []\n",
    "\n",
    "        for k, paragraph in enumerate(chapter):\n",
    "            num_sentences = sentence_counts[i][j][k]\n",
    "\n",
    "            if num_sentences > 0:\n",
    "                paragraph_s_embeddings = sentence_embeddings_flat[current_sentence_idx:current_sentence_idx+num_sentences]\n",
    "                \n",
    "                # paragraph\n",
    "                paragraph_agg_embedding = np.mean(paragraph_s_embeddings, axis= 0)\n",
    "                chapter_paragraph_embeddings.append(paragraph_agg_embedding)\n",
    "                current_sentence_idx += num_sentences\n",
    "\n",
    "        # chapter\n",
    "        if chapter_paragraph_embeddings:\n",
    "            chapter_agg_embedding = np.mean(chapter_paragraph_embeddings, axis=0)\n",
    "            book_chapter_embeddings.append(chapter_agg_embedding)\n",
    "\n",
    "        book_paragraph_embeddings.append(chapter_paragraph_embeddings)\n",
    "# book\n",
    "if book_chapter_embeddings:\n",
    "    book_agg_embedding = np.mean(book_chapter_embeddings, axis=0)\n",
    "    book_embeddings_aggregated.append(book_agg_embedding)\n",
    "\n",
    "paragraph_embeddings_aggregated.append(book_paragraph_embeddings)\n",
    "chapter_embeddings_aggregated.append(book_chapter_embeddings)\n",
    "\n",
    "print(\"\\nAggregation Complete!\")\n",
    "print(f\"Shape of one book embedding (aggregated): {book_embeddings_aggregated[0].shape}\")\n",
    "print(f\"Shape of one chapter embedding (aggregated): {chapter_embeddings_aggregated[0][0].shape}\")\n",
    "print(f\"Shape of one paragraph embedding (aggregated): {paragraph_embeddings_aggregated[0][0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb684b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Creating the Master DataFrame for sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DataFrame: 100%|██████████| 43/43 [00:03<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Adding embeddings and validating...\n",
      "Number of rows in DataFrame: 130122\n",
      "Number of embeddings:        130122\n",
      "\n",
      "DataFrame with sentence embeddings created successfully.\n",
      "                                       book_title            author  year  \\\n",
      "0  A System of Logic, Ratiocinative and Inductive  John Stuart Mill  1843   \n",
      "1  A System of Logic, Ratiocinative and Inductive  John Stuart Mill  1843   \n",
      "2  A System of Logic, Ratiocinative and Inductive  John Stuart Mill  1843   \n",
      "3  A System of Logic, Ratiocinative and Inductive  John Stuart Mill  1843   \n",
      "4  A System of Logic, Ratiocinative and Inductive  John Stuart Mill  1843   \n",
      "\n",
      "   book_index  chapter_index  paragraph_index  sentence_index_in_para  \\\n",
      "0           0              0                0                       0   \n",
      "1           0              0                0                       1   \n",
      "2           0              0                1                       0   \n",
      "3           0              0                1                       1   \n",
      "4           0              0                1                       2   \n",
      "\n",
      "                                                text  \\\n",
      "0  This book makes no pretense of giving to the w...   \n",
      "1  Its claim to attention, if it possess any, is ...   \n",
      "2  To cement together the detached fragments of a...   \n",
      "3  To other originality than this, the present wo...   \n",
      "4  In the existing state of the cultivation of th...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.03259888, 0.073740706, -0.007923428, -0.008...  \n",
      "1  [0.028932719, -0.04044518, 0.0114941355, 0.002...  \n",
      "2  [0.034214858, 0.041414876, -0.013025717, -0.00...  \n",
      "3  [0.043690763, 0.042778708, 0.011360916, 0.0380...  \n",
      "4  [0.05916741, 0.026611779, 0.022938658, -0.0227...  \n",
      "\n",
      "Shape of the DataFrame: (130122, 9)\n",
      "Data type of the 'embedding' column: <class 'numpy.ndarray'>\n",
      "\n",
      "Step 3: Saving the DataFrame to Parquet format...\n",
      "DataFrame successfully saved to sentence_embeddings_with_metadata.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- PREREQUISITES ---\n",
    "# This code assumes you have ALREADY run your embedding generation and have these variables:\n",
    "# - data: The original JSON data loaded from the file.\n",
    "# - sentence_embeddings_flat: The numpy array of embeddings.\n",
    "# - all_sentences_flat: The list of actual sentence strings that were encoded. (Optional but good for validation)\n",
    "\n",
    "# --- 1. Create the Master DataFrame ---\n",
    "print(\"Step 1: Creating the Master DataFrame for sentences...\")\n",
    "\n",
    "# This process must EXACTLY mirror how you created `all_sentences_flat`\n",
    "# We will iterate through the original data and tokenize it into sentences again.\n",
    "records = []\n",
    "for i, book in enumerate(tqdm(data, desc=\"Building DataFrame\")):\n",
    "    book_title = book['meta'].get('Original Title', f'book_{i}')\n",
    "    book_author = book['meta'].get('Original Writer')\n",
    "    year = book['meta'].get('Publication Year (Original)')\n",
    "    for j, chapter in enumerate(book['content']):\n",
    "        for k, paragraph in enumerate(chapter):\n",
    "            # Tokenize the paragraph into sentences, just like before\n",
    "            sentences = nltk.sent_tokenize(paragraph)\n",
    "            \n",
    "            # NOTE: If you used the advanced \"split_text\" function from our\n",
    "            # previous discussion to handle long sentences, you MUST use that\n",
    "            # same logic here to ensure the lists match. For simplicity,\n",
    "            # this example assumes standard sent_tokenize was sufficient.\n",
    "            \n",
    "            if sentences:\n",
    "                for s_idx, sentence in enumerate(sentences):\n",
    "                    records.append({\n",
    "                        \"book_title\": book_title,\n",
    "                        \"author\": book_author,\n",
    "                        \"year\": year,\n",
    "                        \"book_index\": i,\n",
    "                        \"chapter_index\": j,\n",
    "                        \"paragraph_index\": k,\n",
    "                        \"sentence_index_in_para\": s_idx, # Useful for context\n",
    "                        \"text\": sentence\n",
    "                    })\n",
    "\n",
    "# Create the DataFrame from our list of records\n",
    "df_sentences = pd.DataFrame(records)\n",
    "\n",
    "# --- 2. Add Embeddings and Validate ---\n",
    "print(\"\\nStep 2: Adding embeddings and validating...\")\n",
    "\n",
    "# This is the most critical step. The length of the DataFrame must\n",
    "# exactly match the number of embeddings you generated.\n",
    "print(f\"Number of rows in DataFrame: {len(df_sentences)}\")\n",
    "print(f\"Number of embeddings:        {len(sentence_embeddings_flat)}\")\n",
    "\n",
    "assert len(df_sentences) == len(sentence_embeddings_flat), \"Mismatch between DataFrame rows and number of embeddings!\"\n",
    "\n",
    "# Add the pre-computed embeddings as a new column.\n",
    "# We convert the numpy array to a list so pandas can store it.\n",
    "df_sentences['embedding'] = list(sentence_embeddings_flat)\n",
    "\n",
    "print(\"\\nDataFrame with sentence embeddings created successfully.\")\n",
    "print(df_sentences.head())\n",
    "print(f\"\\nShape of the DataFrame: {df_sentences.shape}\")\n",
    "print(f\"Data type of the 'embedding' column: {type(df_sentences['embedding'].iloc[0])}\")\n",
    "\n",
    "\n",
    "print(\"\\nStep 3: Saving the DataFrame to Parquet format...\")\n",
    "\n",
    "# Define the output file path\n",
    "output_path = 'sentence_embeddings_with_metadata.parquet'\n",
    "\n",
    "# Save the DataFrame\n",
    "df_sentences.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"DataFrame successfully saved to {output_path}\")\n",
    "\n",
    "# --- How to load it back later ---\n",
    "# df_loaded = pd.read_parquet(output_path)\n",
    "# print(\"\\nSuccessfully loaded the DataFrame from Parquet:\")\n",
    "# print(df_loaded.head())\n",
    "# print(f\"Embedding type after loading: {type(df_loaded['embedding'].iloc[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39601ec",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766acf3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f75ad0b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1a21679",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c13b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df - > arrow\n",
    "# Assuming you have already run the embedding code and have these variables:\n",
    "# - data: The original JSON data\n",
    "# - paragraph_embeddings_direct_flat: The numpy array of paragraph embeddings\n",
    "\n",
    "print(\"Step 1: Creating the Master DataFrame...\")\n",
    "\n",
    "# Flatten the data structure into a list of dictionaries\n",
    "records = []\n",
    "for i, book in enumerate(data):\n",
    "    book_title = book['meta'].get('Original Title', f'book_{i}')\n",
    "    book_author = book['meta'].get('Original Writer')\n",
    "    for j, chapter in enumerate(book['content']):\n",
    "        for k, paragraph in enumerate(chapter):\n",
    "            # We only create a record if the paragraph is not empty,\n",
    "            # ensuring a 1-to-1 match with our flat embeddings.\n",
    "            if paragraph.strip():\n",
    "                records.append({\n",
    "                    \"book_title\": book_title,\n",
    "                    \"book_index\": i,\n",
    "                    \"author\": book_author\n",
    "                    \"chapter_index\": j,\n",
    "                    \"paragraph_index\": k,\n",
    "                    \"text\": paragraph\n",
    "                })\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Add the pre-computed embeddings as a new column\n",
    "# Ensure the lengths match!\n",
    "assert len(df) == len(paragraph_embeddings_direct_flat)\n",
    "df['embedding'] = list(paragraph_embeddings_direct_flat)\n",
    "\n",
    "print(\"DataFrame created successfully.\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape of the DataFrame: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc13871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paragraph / Chunked Embeddings (Direct) --- NO SENTENCE LEVEL\n",
    "print(\"\\n--- Method 2: Direct Paragraph (Chunk) Embedding ---\")\n",
    "with open(r'00_prep\\cleaned_texts\\all_processed.json', 'r',encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-8B\", device = 'cuda')\n",
    "num_books = len(data)\n",
    "num_chapters = sum(len(book['content']) for book in data)\n",
    "num_paragraphs = sum(len(chapter) for book in data for chapter in book['content'])\n",
    "print(f\"Loaded {num_books} books, {num_chapters} chapters, and {num_paragraphs} paragraphs.\")\n",
    "\n",
    "all_paragraphs_flat = [\n",
    "    paragraph \n",
    "    for book in data \n",
    "    for chapter in book['content'] \n",
    "    for paragraph in chapter \n",
    "    if paragraph.strip() # Ensure paragraph is not empty\n",
    "]\n",
    "\n",
    "print(f\"Total paragraphs/chunks to embed: {len(all_paragraphs_flat)}\")\n",
    "\n",
    "print(\"Generating direct embeddings for all paragraphs...\")\n",
    "paragraph_embeddings_direct_flat = model.encode(\n",
    "    all_paragraphs_flat,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# You can also aggregate these direct paragraph embeddings to get chapter/book level\n",
    "# This is often a better starting point than sentence-level aggregation\n",
    "print(\"\\nAggregating direct paragraph embeddings...\")\n",
    "# We need to structure them hierarchically first\n",
    "paragraph_embeddings_direct = []\n",
    "chapter_embeddings_from_direct = []\n",
    "book_embeddings_from_direct = []\n",
    "\n",
    "current_paragraph_idx = 0\n",
    "for book in tqdm(data, desc=\"Aggregating Direct Paragraphs\"):\n",
    "    book_chapter_embs = []\n",
    "    book_para_embs = []\n",
    "    for chapter in book['content']:\n",
    "        num_paragraphs_in_chapter = len([p for p in chapter if p.strip()])\n",
    "        if num_paragraphs_in_chapter > 0:\n",
    "            # Slice the embeddings for the current chapter\n",
    "            chapter_para_embs = paragraph_embeddings_direct_flat[current_paragraph_idx : current_paragraph_idx + num_paragraphs_in_chapter]\n",
    "            \n",
    "            # --- CHAPTER EMBEDDING (from direct chunks) ---\n",
    "            chapter_agg_emb = np.mean(chapter_para_embs, axis=0)\n",
    "            book_chapter_embs.append(chapter_agg_emb)\n",
    "            book_para_embs.append(chapter_para_embs)\n",
    "            current_paragraph_idx += num_paragraphs_in_chapter\n",
    "    \n",
    "    # --- BOOK EMBEDDING (from direct chunks) ---\n",
    "    if book_chapter_embs:\n",
    "        book_agg_emb = np.mean(book_chapter_embs, axis=0)\n",
    "        book_embeddings_from_direct.append(book_agg_emb)\n",
    "        \n",
    "    chapter_embeddings_from_direct.append(book_chapter_embs)\n",
    "    paragraph_embeddings_direct.append(book_para_embs)\n",
    "\n",
    "print(\"\\nDirect Embedding and Aggregation Complete!\")\n",
    "print(f\"Shape of one book embedding (from direct): {book_embeddings_from_direct[0].shape}\")\n",
    "print(f\"Shape of one chapter embedding (from direct): {chapter_embeddings_from_direct[0][0].shape}\")\n",
    "print(f\"Shape of one paragraph embedding (direct): {paragraph_embeddings_direct[0][0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99efef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df - > arrow\n",
    "# Assuming you have already run the embedding code and have these variables:\n",
    "# - data: The original JSON data\n",
    "# - paragraph_embeddings_direct_flat: The numpy array of paragraph embeddings\n",
    "\n",
    "print(\"Step 1: Creating the Master DataFrame...\")\n",
    "\n",
    "# Flatten the data structure into a list of dictionaries\n",
    "records = []\n",
    "for i, book in enumerate(data):\n",
    "    book_title = book['meta'].get('Original Title', f'book_{i}')\n",
    "    book_author = book['meta'].get('Original Writer')\n",
    "    for j, chapter in enumerate(book['content']):\n",
    "        for k, paragraph in enumerate(chapter):\n",
    "            # We only create a record if the paragraph is not empty,\n",
    "            # ensuring a 1-to-1 match with our flat embeddings.\n",
    "            if paragraph.strip():\n",
    "                records.append({\n",
    "                    \"book_title\": book_title,\n",
    "                    \"book_index\": i,\n",
    "                    \"author\": book_author\n",
    "                    \"chapter_index\": j,\n",
    "                    \"paragraph_index\": k,\n",
    "                    \"text\": paragraph\n",
    "                })\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Add the pre-computed embeddings as a new column\n",
    "# Ensure the lengths match!\n",
    "assert len(df) == len(paragraph_embeddings_direct_flat)\n",
    "df['embedding'] = list(paragraph_embeddings_direct_flat)\n",
    "\n",
    "print(\"DataFrame created successfully.\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape of the DataFrame: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE GRAINED topic modeling of main cluster... \n",
    "docs_subset = [...]\n",
    "embeddings_subset = [...]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "logger.info(\"Starting fine-grained sub-clustering with sensitive parameters...\")\n",
    "\n",
    "# --- Create a \"Microscope\" BERTopic Model ---\n",
    "\n",
    "# 1. Define highly sensitive UMAP and HDBSCAN models\n",
    "sensitive_umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "\n",
    "sensitive_hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=10,\n",
    "    min_samples=5,\n",
    "    metric='cosine',\n",
    "    cluster_selection_method='leaf', # Use 'leaf' for fine-grained topics\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# 2. Instantiate a new BERTopic model with these components\n",
    "sub_topic_model = BERTopic(\n",
    "    umap_model=sensitive_umap_model,\n",
    "    hdbscan_model=sensitive_hdbscan_model,\n",
    "    language=\"english\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 3. Fit the model on your subset\n",
    "sub_topics, _ = sub_topic_model.fit_transform(docs_subset, embeddings=embeddings_subset)\n",
    "\n",
    "# 4. Analyze the results\n",
    "print(sub_topic_model.get_topic_info())\n",
    "\n",
    "# --- Iterate and Refine ---\n",
    "# If you get too many tiny topics, slightly increase min_cluster_size (e.g., to 15 or 20).\n",
    "# If the topics are still too broad, slightly decrease n_neighbors (e.g., to 5).\n",
    "# This interactive process is much more effective than re-running the whole Optuna search.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ?????????\n",
    "# with custom vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a vectorizer that IGNORES default stopwords\n",
    "# and instead focuses on words specific to your philosophy corpus.\n",
    "custom_vectorizer = CountVectorizer(\n",
    "    stop_words=None, # No default stop words\n",
    "    min_df=5,        # Word must appear in at least 5 paragraphs to be considered\n",
    "    max_df=0.8       # Ignore words that appear in >80% of paragraphs (like 'the', 'is', if they weren't already filtered)\n",
    ")\n",
    "\n",
    "# Then plug this into your sub_topic_model\n",
    "sub_topic_model = BERTopic(\n",
    "    #...\n",
    "    vectorizer_model=custom_vectorizer,\n",
    "    #...\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
