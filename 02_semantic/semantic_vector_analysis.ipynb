{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91dd8a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to GPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# random seed  for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    \n",
    "MODEL_DIR = \"E:/code/dta/cls/exam/histbert\" \n",
    "model = BertModel.from_pretrained(MODEL_DIR)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "model.eval() \n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "    print(\"Model moved to GPU.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d7b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_for_timeslice(target_word, sentences, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generates a single aggregated embedding for a target word from a list of sentences.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # 1. Tokenize the sentence and find the token(s) for our target word\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "        input_ids = inputs[\"input_ids\"][0]\n",
    "\n",
    "        # Find all occurrences of the target word's token(s)\n",
    "        # Note: Words can be split into multiple subword tokens (e.g., \"broadcast\" -> \"broad\", \"##cast\")\n",
    "        target_tokens = tokenizer.tokenize(target_word)\n",
    "        target_token_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        \n",
    "        token_indices = []\n",
    "        for i in range(len(input_ids) - len(target_token_ids) + 1):\n",
    "            if list(input_ids[i:i+len(target_token_ids)]) == target_token_ids:\n",
    "                token_indices.extend(range(i, i + len(target_token_ids)))\n",
    "\n",
    "        if not token_indices:\n",
    "            continue\n",
    "\n",
    "        # 2. Get the model's hidden states (the embeddings)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            hidden_states = outputs.hidden_states[-2] \n",
    "            \n",
    "        # 3. Extract and average the embeddings for our target word's tokens\n",
    "        word_embedding = hidden_states[0, token_indices, :].mean(dim=0)\n",
    "        all_embeddings.append(word_embedding)\n",
    "\n",
    "    if not all_embeddings:\n",
    "        return None # Return None if the word was not found in any sentence\n",
    "\n",
    "    # 4. Aggregate all contextual instances into a single vector\n",
    "    final_embedding = torch.stack(all_embeddings).mean(dim=0)\n",
    "    \n",
    "    return final_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d6a3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Nothing is more usual and more natural for those, who pretend to discover\n",
    " anything new to the world in philosophy and the sciences, than to\n",
    " insinuate the praises of their own systems, by decrying all those, which\n",
    " have been advanced before them.\"\"\"\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9579de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2498,  2003,  2062,  5156,  1998,  2062,  3019,  2005,  2216,\n",
       "          1010,  2040,  9811,  2000,  7523,  2505,  2047,  2000,  1996,  2088,\n",
       "          1999,  4695,  1998,  1996,  4163,  1010,  2084,  2000, 16021,  2378,\n",
       "         20598,  1996, 27128,  1997,  2037,  2219,  3001,  1010,  2011, 11703,\n",
       "          2854,  2075,  2035,  2216,  1010,  2029,  2031,  2042,  3935,  2077,\n",
       "          2068,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "text = \"GeeksforGeeks is a computer science portal\"\n",
    "\n",
    "# Tokenize and encode text using batch_encode_plus\n",
    "# The function returns a dictionary containing the token IDs and attention masks\n",
    "encoding = tokenizer.batch_encode_plus( [text],# List of input texts\n",
    "    padding=True,              # Pad to the maximum sequence length\n",
    "    truncation=True,           # Truncate to the maximum sequence length if necessary\n",
    "    return_tensors='pt',      # Return PyTorch tensors\n",
    "    add_special_tokens=True    # Add special tokens CLS and SEP\n",
    ")\n",
    "\n",
    "input_ids = encoding['input_ids']  # Token IDs\n",
    "# print input IDs\n",
    "print(f\"Input ID: {input_ids}\")\n",
    "attention_mask = encoding['attention_mask']  # Attention mask\n",
    "# print attention mask\n",
    "print(f\"Attention mask: {attention_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b228523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using BERT model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    word_embeddings = outputs.last_hidden_state  # This contains the embeddings\n",
    "\n",
    "# Output the shape of word embeddings\n",
    "print(f\"Shape of Word Embeddings: {word_embeddings.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
