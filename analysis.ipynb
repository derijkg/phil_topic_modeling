{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26109f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import json\n",
    "with open('data_processing\\data.json','r',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119fb1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_df word freq analysis\n",
    "# Example:\n",
    "\n",
    "\n",
    "# Assuming 'processed_series' is your lemmatized/preprocessed text\n",
    "# vec_temp = CountVectorizer(lowercase=True, stop_words='english')\n",
    "# dtm_temp = vec_temp.fit_transform(processed_series)\n",
    "# doc_freq = (dtm_temp > 0).sum(axis=0) # Counts documents where term > 0\n",
    "# doc_freq = pd.Series(doc_freq.A1, index=vec_temp.get_feature_names_out()) # .A1 to flatten\n",
    "\n",
    "# print(\"Document Frequencies (Top & Bottom):\")\n",
    "# print(doc_freq.sort_values(ascending=False).head())\n",
    "# print(doc_freq.sort_values(ascending=True).head(20)) # Look at very rare words\n",
    "\n",
    "# plt.figure(figsize=(10,5))\n",
    "# doc_freq.hist(bins=50, range=(0, doc_freq.quantile(0.95))) # Plot up to 95th percentile\n",
    "# plt.title(\"Distribution of Document Frequencies\")\n",
    "# plt.xlabel(\"Number of Documents Term Appears In\")\n",
    "# plt.ylabel(\"Number of Terms\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# corpus size:\n",
    "#   small(<1000 docs) -> min_df =2 / 3\n",
    "#   large 100 000 docs -> 5/10 or float (0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf04a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_df: ignore terms that appear in more than max)df docs\n",
    "# corpus specific stop words\n",
    "\n",
    "#analysis\n",
    "#as above + v\n",
    "# print(doc_freq.sort_values(ascending=False)/head(20))\n",
    "\n",
    "#typical values\n",
    "#   0.70, 0.95 (lower the more focused the corpus)\n",
    "# if too low might remove high level concept words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfae6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words + my_custom_stop_words\n",
    "# add if topics are dominated by words which are not useful\n",
    "\n",
    "#analysis:\n",
    "#   run lda with basic preprcossing look at top words of each topic\n",
    "#   words that appear in many top topic lists, but arent discrimiations are key candidates\n",
    "# custom_stop_words = ['figure', 'table', 'et', 'al', 'also', 'however', 'may', 'might', 'study', 'results']\n",
    "# # Add more based on your data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4459df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ngram_range:\n",
    "What it is: Tuple (min_n, max_n) for n-gram range. E.g., (1, 1) for unigrams only, (1, 2) for unigrams and bigrams.\n",
    "Why tune it for your data:\n",
    "Capturing Phrases: Your data might contain important multi-word expressions (e.g., \"machine learning,\" \"New York,\" \"climate change\"). Bigrams or trigrams can capture these.\n",
    "How to analyze your data:\n",
    "Look for Common Phrases: Read through a sample of your documents. Are there recurring meaningful phrases?\n",
    "PMI or Chi-Squared for Collocations: More advanced: use statistical measures to find significant collocations (phrases that appear together more often than by chance). Libraries like nltk can help.\n",
    "Impact on Vocabulary: N-grams drastically increase vocabulary size. Use with min_df to prune infrequent n-grams.\n",
    "Start with (1,1) or (1,2). (1,3) can be useful but often adds too much noise unless min_df is set appropriately high for trigrams.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457bbd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lemma or stemming (pos tagging)\n",
    "\n",
    "Lemmatization vs. Stemming (and POS tagging):\n",
    "What it is: Reducing words to their root form. POS tagging identifies the part of speech.\n",
    "Why tune it for your data:\n",
    "Lemmatization (e.g., \"running\" -> \"run\") is generally preferred over stemming (e.g., \"running\" -> \"run\", \"studies\" -> \"studi\") as it produces actual words and is less aggressive.\n",
    "POS Tag Filtering: For some datasets, keeping only nouns, or nouns and adjectives, can yield cleaner topics by removing verbs or adverbs that might be too generic.\n",
    "How to analyze your data:\n",
    "Nature of Language: Is your text formal or informal? Does it have many word variations?\n",
    "Experiment: Try with and without lemmatization. Try filtering by POS tags (e.g., allowed_postags=['NOUN', 'ADJ'] in your lemmatize_spacy_chunked). Compare topic coherence and interpretability.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a46fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lda model parameters\n",
    "\n",
    "2. LDA Model Parameters\n",
    "n_components (Number of Topics):\n",
    "What it is: The desired number of topics.\n",
    "Why tune it for your data: This is the most critical LDA parameter. Too few, and topics will be too broad and mixed. Too many, and topics might become too granular, repetitive, or capture noise.\n",
    "How to analyze your data (and model output):\n",
    "Coherence Scores: As discussed, plot coherence (e.g., C_v) against a range of n_components. Look for a peak or an \"elbow point\" where coherence stops increasing significantly.\n",
    "Qualitative Inspection: For promising n_components values from the coherence plot, manually inspect the topics.\n",
    "Are they distinct and interpretable?\n",
    "Are there too many \"junk\" topics?\n",
    "Are some meaningful themes split across too many topics?\n",
    "Domain Knowledge: How many natural themes or sub-themes do you expect to find in your data? This can be a rough guide.\n",
    "Held-out Perplexity (Less Reliable for Interpretability): Sometimes used, but lower perplexity doesn't always mean more human-interpretable topics.\n",
    "doc_topic_prior (alpha) and topic_word_prior (beta/eta):\n",
    "What they are: Dirichlet priors controlling the sparsity/smoothness of distributions.\n",
    "alpha: Document-topic distribution. Lower alpha = documents are represented by fewer topics (sparser). Higher alpha = documents are mixtures of more topics (smoother).\n",
    "beta (or eta in some libraries): Topic-word distribution. Lower beta = topics are represented by fewer words (sparser). Higher beta = topics are mixtures of more words (smoother).\n",
    "Why tune it for your data:\n",
    "The defaults (1/n_components for alpha, 1/n_components for beta in sklearn, or often 0.1 or 0.01 in other libraries) work reasonably well.\n",
    "If your documents are very short and focused on single themes, a lower alpha might be better.\n",
    "If topics seem too \"diffuse\" (many words with low probabilities), try a slightly lower beta. If topics are too sparse and miss related words, try a slightly higher beta.\n",
    "How to analyze your data (and model output):\n",
    "Document Lengths: Are your documents short or long? More diverse or focused?\n",
    "Topic Sparsity: After an initial run, look at the topic-word distributions. Do topics have a few dominant words, or are weights spread thinly?\n",
    "Document-Topic Sparsity: Look at document_topic_distributions_df. Are documents typically assigned strongly to one or two topics, or are probabilities spread across many?\n",
    "Usually Tuned After n_components: Get n_components right first. Then, if needed, experiment with alpha and beta. Common values to try are 0.1, 0.01, 0.5, or using \"symmetric\" vs. \"asymmetric\" if your library supports it (sklearn's default is symmetric). gensim offers more flexibility for asymmetric priors.\n",
    "learning_method ('online' vs. 'batch'):\n",
    "'online': Faster for large datasets, processes mini-batches.\n",
    "'batch': Processes all data in each iteration. Can be more accurate for smaller datasets but slower.\n",
    "How to analyze your data:\n",
    "Dataset Size: For very large datasets, 'online' is often necessary. For small to medium, 'batch' might give slightly better results if time permits.\n",
    "random_state:\n",
    "Not data-specific for tuning, but crucial for reproducibility. Always set it.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01722bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b968f9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb75ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8389d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "bertopic parameter tuning\n",
    "\n",
    "can also use gensim coherence model but its designed for LDA ~ different preprocessing\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303985b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic diversity , reasonable diversity and low outliers\n",
    "def calculate_topic_diversity(topic_word_lists):\n",
    "    if not topic_word_lists or len(topic_word_lists) == 0:\n",
    "        return 0.0\n",
    "    unique_words = set()\n",
    "    total_words = 0\n",
    "    for topic in topic_word_lists:\n",
    "        unique_words.update(topic)\n",
    "        total_words += len(topic)\n",
    "    if total_words == 0: return 0.0\n",
    "    return len(unique_words) / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for embeddings vs clusters\n",
    "\n",
    "# silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "# Assuming 'embeddings' are your document embeddings\n",
    "# and 'labels' are topic_model.topics_\n",
    "valid_indices = [i for i, label in enumerate(topic_model.topics_) if label != -1]\n",
    "if len(valid_indices) > 1 and len(set(labels[i] for i in valid_indices)) > 1: # Need at least 2 clusters\n",
    "    valid_embeddings = embeddings[valid_indices]\n",
    "    valid_labels = [topic_model.topics_[i] for i in valid_indices]\n",
    "    if len(set(valid_labels)) > 1: # Ensure more than one cluster for silhouette\n",
    "         score = silhouette_score(valid_embeddings, valid_labels)\n",
    "         print(f\"Silhouette Score: {score}\")\n",
    "\n",
    "\n",
    "\n",
    "# davies bouldin index: embeddings vs clusters\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "# ... (get valid_embeddings, valid_labels as above) ...\n",
    "if len(set(valid_labels)) > 1: # Ensure more than one cluster\n",
    "    score = davies_bouldin_score(valid_embeddings, valid_labels)\n",
    "    print(f\"Davies-Bouldin Index: {score}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
